{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "object_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaYMLhYFNlczYBO0j2bT8B"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KHe5d32pPZBz"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu3XtquRPjqv",
        "outputId": "b482612e-52ad-4d9e-94ef-317e3b1705f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 13s 0us/step\n",
            "170508288/170498071 [==============================] - 13s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets determine the dataset characteristics\n",
        "print('Training Images: {}'.format(X_train.shape))\n",
        "print('Testing Images: {}'.format(X_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJfBMIvePzDZ",
        "outputId": "7b286a1b-56c9-4dea-e351-e655bead45c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Images: (50000, 32, 32, 3)\n",
            "Testing Images: (10000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now for a single image \n",
        "print(X_train[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J_kAPvrQeAh",
        "outputId": "352667fe-4b47-4f29-b91d-b25544f8d93a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n"
      ],
      "metadata": {
        "id": "80NZtIh6Qscl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a convolutional neural network for object recognition on CIFAR-10\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 6\n",
        "np.random.seed(seed) \n",
        "\n",
        "# load the data\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# normalize the inputs from 0-255 to 0.0-1.0\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "metadata": {
        "id": "HvwnQC3JRWud"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class labels shape\n",
        "print(y_train.shape)\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onpYealqSA0n",
        "outputId": "5ac07f78-d377-4741-a261-0de3002fd268"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 1)\n",
            "[6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hot encode outputs\n",
        "Y_train = np_utils.to_categorical(y_train)\n",
        "Y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = Y_test.shape[1]\n",
        "\n",
        "print(Y_train.shape)\n",
        "print(Y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWhIEcrDSFls",
        "outputId": "3896f4a0-1416-4aae-e2fc-ae7e68dee7e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 10)\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#builiding allcnn\n"
      ],
      "metadata": {
        "id": "PW8J_LqESJXb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start building the model - import necessary layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Activation, Conv2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "def allcnn(weights=None):\n",
        "    # define model type - Sequential\n",
        "    model = Sequential()\n",
        "\n",
        "    # add model layers - Convolution2D, Activation, Dropout\n",
        "    model.add(Conv2D(96, (3, 3), padding = 'same', input_shape=(32,32,3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(96, (3, 3), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(96, (3, 3), padding = 'same', strides = (2,2)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Conv2D(192, (3, 3), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(192, (3, 3), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(192, (3, 3), padding = 'same', strides = (2,2)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Conv2D(192, (3, 3), padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(192, (1, 1), padding = 'valid'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(10, (1, 1), padding = 'valid'))\n",
        "\n",
        "    # add GlobalAveragePooling2D layer with Softmax activation\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    # load the weights\n",
        "    if weights:\n",
        "        model.load_weights(weights)\n",
        "    \n",
        "    # return model\n",
        "    return model"
      ],
      "metadata": {
        "id": "kLhKhULPSUov"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Defining Parameters and Training the Model"
      ],
      "metadata": {
        "id": "tss7H6NkSdTY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define hyper parameters\n",
        "learning_rate = 0.01\n",
        "weight_decay = 1e-6\n",
        "momentum = 0.9\n",
        "\n",
        "# build model \n",
        "model = allcnn()\n",
        "\n",
        "# define optimizer and compile model\n",
        "sgd = SGD(lr=learning_rate, decay=weight_decay, momentum=momentum, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# print model summary\n",
        "print (model.summary())\n",
        "\n",
        "# define additional training parameters\n",
        "epochs = 350\n",
        "batch_size = 32\n",
        "\n",
        "# fit the model\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=batch_size, verbose = 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31rAKxFGSjx1",
        "outputId": "c385d5fc-5bb8-48f5-d22d-c485cf7435a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 96)        2688      \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 96)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 96)        83040     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 32, 32, 96)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 96)        83040     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 96)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 192)       166080    \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 16, 16, 192)       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 192)       331968    \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 16, 16, 192)       0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 192)         331968    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 8, 8, 192)         331968    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 8, 8, 192)         37056     \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 8, 8, 192)         0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 10)          1930      \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 10)               0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,369,738\n",
            "Trainable params: 1,369,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/350\n",
            "1563/1563 [==============================] - 40s 18ms/step - loss: 2.0369 - accuracy: 0.2274 - val_loss: 1.6979 - val_accuracy: 0.3535\n",
            "Epoch 2/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 1.5406 - accuracy: 0.4260 - val_loss: 1.3660 - val_accuracy: 0.4918\n",
            "Epoch 3/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 1.2560 - accuracy: 0.5467 - val_loss: 1.1129 - val_accuracy: 0.6003\n",
            "Epoch 4/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 1.0540 - accuracy: 0.6229 - val_loss: 0.9335 - val_accuracy: 0.6701\n",
            "Epoch 5/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.9143 - accuracy: 0.6740 - val_loss: 0.8943 - val_accuracy: 0.6923\n",
            "Epoch 6/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.8021 - accuracy: 0.7194 - val_loss: 0.7839 - val_accuracy: 0.7244\n",
            "Epoch 7/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.7177 - accuracy: 0.7463 - val_loss: 0.7205 - val_accuracy: 0.7543\n",
            "Epoch 8/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.6508 - accuracy: 0.7720 - val_loss: 0.6715 - val_accuracy: 0.7671\n",
            "Epoch 9/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.6007 - accuracy: 0.7894 - val_loss: 0.6573 - val_accuracy: 0.7761\n",
            "Epoch 10/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.5514 - accuracy: 0.8060 - val_loss: 0.6583 - val_accuracy: 0.7836\n",
            "Epoch 11/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.5154 - accuracy: 0.8186 - val_loss: 0.6089 - val_accuracy: 0.8006\n",
            "Epoch 12/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.4730 - accuracy: 0.8335 - val_loss: 0.5849 - val_accuracy: 0.8127\n",
            "Epoch 13/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.4475 - accuracy: 0.8426 - val_loss: 0.6583 - val_accuracy: 0.8026\n",
            "Epoch 14/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.4156 - accuracy: 0.8538 - val_loss: 0.6062 - val_accuracy: 0.8097\n",
            "Epoch 15/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.3950 - accuracy: 0.8601 - val_loss: 0.6170 - val_accuracy: 0.8039\n",
            "Epoch 16/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.3758 - accuracy: 0.8677 - val_loss: 0.6009 - val_accuracy: 0.8055\n",
            "Epoch 17/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.3552 - accuracy: 0.8740 - val_loss: 0.5980 - val_accuracy: 0.8073\n",
            "Epoch 18/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.3398 - accuracy: 0.8778 - val_loss: 0.6324 - val_accuracy: 0.8098\n",
            "Epoch 19/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.3315 - accuracy: 0.8834 - val_loss: 0.6651 - val_accuracy: 0.8017\n",
            "Epoch 20/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.3070 - accuracy: 0.8911 - val_loss: 0.6890 - val_accuracy: 0.8134\n",
            "Epoch 21/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2980 - accuracy: 0.8934 - val_loss: 0.6589 - val_accuracy: 0.8172\n",
            "Epoch 22/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.2915 - accuracy: 0.8956 - val_loss: 0.6764 - val_accuracy: 0.8135\n",
            "Epoch 23/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2846 - accuracy: 0.8979 - val_loss: 0.6331 - val_accuracy: 0.8217\n",
            "Epoch 24/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2733 - accuracy: 0.9027 - val_loss: 0.6998 - val_accuracy: 0.8188\n",
            "Epoch 25/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.2650 - accuracy: 0.9069 - val_loss: 0.6824 - val_accuracy: 0.8160\n",
            "Epoch 26/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2679 - accuracy: 0.9047 - val_loss: 0.6596 - val_accuracy: 0.8179\n",
            "Epoch 27/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2511 - accuracy: 0.9102 - val_loss: 0.7236 - val_accuracy: 0.8089\n",
            "Epoch 28/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.2480 - accuracy: 0.9117 - val_loss: 0.7743 - val_accuracy: 0.8122\n",
            "Epoch 29/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2423 - accuracy: 0.9140 - val_loss: 0.7967 - val_accuracy: 0.8080\n",
            "Epoch 30/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2424 - accuracy: 0.9145 - val_loss: 0.6981 - val_accuracy: 0.8199\n",
            "Epoch 31/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2367 - accuracy: 0.9161 - val_loss: 0.6914 - val_accuracy: 0.8125\n",
            "Epoch 32/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2247 - accuracy: 0.9226 - val_loss: 0.6899 - val_accuracy: 0.8152\n",
            "Epoch 33/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2317 - accuracy: 0.9182 - val_loss: 0.7382 - val_accuracy: 0.8255\n",
            "Epoch 34/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2239 - accuracy: 0.9218 - val_loss: 0.7557 - val_accuracy: 0.8142\n",
            "Epoch 35/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2270 - accuracy: 0.9206 - val_loss: 0.7519 - val_accuracy: 0.8194\n",
            "Epoch 36/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2178 - accuracy: 0.9235 - val_loss: 0.8213 - val_accuracy: 0.8217\n",
            "Epoch 37/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2151 - accuracy: 0.9247 - val_loss: 0.8686 - val_accuracy: 0.8104\n",
            "Epoch 38/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.2160 - accuracy: 0.9246 - val_loss: 0.7529 - val_accuracy: 0.8287\n",
            "Epoch 39/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.2142 - accuracy: 0.9243 - val_loss: 0.7684 - val_accuracy: 0.8197\n",
            "Epoch 40/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.2165 - accuracy: 0.9249 - val_loss: 0.8653 - val_accuracy: 0.8168\n",
            "Epoch 41/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2148 - accuracy: 0.9255 - val_loss: 0.7312 - val_accuracy: 0.8321\n",
            "Epoch 42/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2025 - accuracy: 0.9289 - val_loss: 0.8305 - val_accuracy: 0.8116\n",
            "Epoch 43/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.2067 - accuracy: 0.9284 - val_loss: 0.7954 - val_accuracy: 0.8188\n",
            "Epoch 44/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.2049 - accuracy: 0.9298 - val_loss: 0.8109 - val_accuracy: 0.8228\n",
            "Epoch 45/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2017 - accuracy: 0.9301 - val_loss: 0.7882 - val_accuracy: 0.8279\n",
            "Epoch 46/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1961 - accuracy: 0.9319 - val_loss: 0.9071 - val_accuracy: 0.8187\n",
            "Epoch 47/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1984 - accuracy: 0.9303 - val_loss: 0.9235 - val_accuracy: 0.8253\n",
            "Epoch 48/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.2012 - accuracy: 0.9310 - val_loss: 0.8526 - val_accuracy: 0.8262\n",
            "Epoch 49/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1915 - accuracy: 0.9337 - val_loss: 0.8683 - val_accuracy: 0.8217\n",
            "Epoch 50/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1935 - accuracy: 0.9323 - val_loss: 0.8488 - val_accuracy: 0.8256\n",
            "Epoch 51/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1926 - accuracy: 0.9348 - val_loss: 0.8198 - val_accuracy: 0.8203\n",
            "Epoch 52/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.2016 - accuracy: 0.9321 - val_loss: 0.8110 - val_accuracy: 0.8179\n",
            "Epoch 53/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1896 - accuracy: 0.9348 - val_loss: 0.9373 - val_accuracy: 0.8193\n",
            "Epoch 54/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1972 - accuracy: 0.9313 - val_loss: 0.8414 - val_accuracy: 0.8252\n",
            "Epoch 55/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1860 - accuracy: 0.9359 - val_loss: 0.9903 - val_accuracy: 0.8250\n",
            "Epoch 56/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1918 - accuracy: 0.9352 - val_loss: 0.8985 - val_accuracy: 0.8278\n",
            "Epoch 57/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1893 - accuracy: 0.9358 - val_loss: 0.9269 - val_accuracy: 0.8210\n",
            "Epoch 58/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1733 - accuracy: 0.9412 - val_loss: 1.0448 - val_accuracy: 0.8152\n",
            "Epoch 59/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1875 - accuracy: 0.9358 - val_loss: 0.8586 - val_accuracy: 0.8323\n",
            "Epoch 60/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1806 - accuracy: 0.9382 - val_loss: 0.8905 - val_accuracy: 0.8229\n",
            "Epoch 61/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1851 - accuracy: 0.9365 - val_loss: 0.9392 - val_accuracy: 0.8248\n",
            "Epoch 62/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1910 - accuracy: 0.9350 - val_loss: 0.9044 - val_accuracy: 0.8189\n",
            "Epoch 63/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1750 - accuracy: 0.9400 - val_loss: 0.8778 - val_accuracy: 0.8287\n",
            "Epoch 64/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1845 - accuracy: 0.9382 - val_loss: 0.8402 - val_accuracy: 0.8275\n",
            "Epoch 65/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1887 - accuracy: 0.9366 - val_loss: 0.9165 - val_accuracy: 0.8195\n",
            "Epoch 66/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1812 - accuracy: 0.9394 - val_loss: 0.8752 - val_accuracy: 0.8218\n",
            "Epoch 67/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1794 - accuracy: 0.9388 - val_loss: 1.0258 - val_accuracy: 0.8170\n",
            "Epoch 68/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1823 - accuracy: 0.9382 - val_loss: 0.9825 - val_accuracy: 0.8152\n",
            "Epoch 69/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1836 - accuracy: 0.9388 - val_loss: 0.9148 - val_accuracy: 0.8227\n",
            "Epoch 70/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1836 - accuracy: 0.9373 - val_loss: 0.8961 - val_accuracy: 0.8237\n",
            "Epoch 71/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1782 - accuracy: 0.9396 - val_loss: 1.0637 - val_accuracy: 0.8143\n",
            "Epoch 72/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1846 - accuracy: 0.9380 - val_loss: 1.1534 - val_accuracy: 0.8119\n",
            "Epoch 73/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1841 - accuracy: 0.9391 - val_loss: 0.9720 - val_accuracy: 0.8209\n",
            "Epoch 74/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1814 - accuracy: 0.9403 - val_loss: 0.9388 - val_accuracy: 0.8295\n",
            "Epoch 75/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1863 - accuracy: 0.9376 - val_loss: 1.0360 - val_accuracy: 0.8186\n",
            "Epoch 76/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1859 - accuracy: 0.9386 - val_loss: 0.9733 - val_accuracy: 0.8266\n",
            "Epoch 77/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1858 - accuracy: 0.9377 - val_loss: 1.0495 - val_accuracy: 0.8306\n",
            "Epoch 78/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1803 - accuracy: 0.9395 - val_loss: 0.9482 - val_accuracy: 0.8152\n",
            "Epoch 79/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1877 - accuracy: 0.9368 - val_loss: 1.0181 - val_accuracy: 0.8222\n",
            "Epoch 80/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1816 - accuracy: 0.9402 - val_loss: 0.9959 - val_accuracy: 0.8335\n",
            "Epoch 81/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1781 - accuracy: 0.9407 - val_loss: 1.0537 - val_accuracy: 0.8163\n",
            "Epoch 82/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1812 - accuracy: 0.9407 - val_loss: 1.1445 - val_accuracy: 0.8094\n",
            "Epoch 83/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1784 - accuracy: 0.9407 - val_loss: 0.9879 - val_accuracy: 0.8179\n",
            "Epoch 84/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1870 - accuracy: 0.9387 - val_loss: 0.9376 - val_accuracy: 0.8266\n",
            "Epoch 85/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1899 - accuracy: 0.9371 - val_loss: 1.0732 - val_accuracy: 0.8147\n",
            "Epoch 86/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1890 - accuracy: 0.9384 - val_loss: 0.9478 - val_accuracy: 0.8217\n",
            "Epoch 87/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1781 - accuracy: 0.9418 - val_loss: 1.0176 - val_accuracy: 0.8286\n",
            "Epoch 88/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1808 - accuracy: 0.9406 - val_loss: 0.9735 - val_accuracy: 0.8208\n",
            "Epoch 89/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1723 - accuracy: 0.9425 - val_loss: 1.0139 - val_accuracy: 0.8133\n",
            "Epoch 90/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1817 - accuracy: 0.9402 - val_loss: 0.9553 - val_accuracy: 0.8296\n",
            "Epoch 91/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1780 - accuracy: 0.9410 - val_loss: 0.9924 - val_accuracy: 0.8170\n",
            "Epoch 92/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1763 - accuracy: 0.9422 - val_loss: 1.0364 - val_accuracy: 0.8220\n",
            "Epoch 93/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1835 - accuracy: 0.9392 - val_loss: 1.1029 - val_accuracy: 0.8038\n",
            "Epoch 94/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1822 - accuracy: 0.9391 - val_loss: 0.9926 - val_accuracy: 0.8216\n",
            "Epoch 95/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1798 - accuracy: 0.9402 - val_loss: 1.1469 - val_accuracy: 0.8161\n",
            "Epoch 96/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1762 - accuracy: 0.9420 - val_loss: 1.0573 - val_accuracy: 0.8277\n",
            "Epoch 97/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1882 - accuracy: 0.9383 - val_loss: 1.1678 - val_accuracy: 0.8176\n",
            "Epoch 98/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1930 - accuracy: 0.9359 - val_loss: 0.9795 - val_accuracy: 0.8263\n",
            "Epoch 99/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1920 - accuracy: 0.9356 - val_loss: 1.0599 - val_accuracy: 0.8229\n",
            "Epoch 100/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1809 - accuracy: 0.9401 - val_loss: 1.0959 - val_accuracy: 0.8189\n",
            "Epoch 101/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1873 - accuracy: 0.9387 - val_loss: 1.0456 - val_accuracy: 0.8170\n",
            "Epoch 102/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1836 - accuracy: 0.9398 - val_loss: 1.0477 - val_accuracy: 0.8205\n",
            "Epoch 103/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1749 - accuracy: 0.9415 - val_loss: 1.2177 - val_accuracy: 0.8128\n",
            "Epoch 104/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1873 - accuracy: 0.9393 - val_loss: 1.0071 - val_accuracy: 0.8191\n",
            "Epoch 105/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1829 - accuracy: 0.9395 - val_loss: 1.0698 - val_accuracy: 0.8217\n",
            "Epoch 106/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1800 - accuracy: 0.9411 - val_loss: 1.0308 - val_accuracy: 0.8247\n",
            "Epoch 107/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1645 - accuracy: 0.9463 - val_loss: 1.1006 - val_accuracy: 0.8224\n",
            "Epoch 108/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1751 - accuracy: 0.9425 - val_loss: 1.0579 - val_accuracy: 0.8236\n",
            "Epoch 109/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1750 - accuracy: 0.9424 - val_loss: 1.1228 - val_accuracy: 0.8269\n",
            "Epoch 110/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1888 - accuracy: 0.9390 - val_loss: 0.9937 - val_accuracy: 0.8298\n",
            "Epoch 111/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1714 - accuracy: 0.9442 - val_loss: 1.0930 - val_accuracy: 0.8180\n",
            "Epoch 112/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1795 - accuracy: 0.9408 - val_loss: 1.1712 - val_accuracy: 0.8236\n",
            "Epoch 113/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1754 - accuracy: 0.9435 - val_loss: 1.0175 - val_accuracy: 0.8196\n",
            "Epoch 114/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1758 - accuracy: 0.9433 - val_loss: 1.0625 - val_accuracy: 0.8227\n",
            "Epoch 115/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1758 - accuracy: 0.9424 - val_loss: 1.0667 - val_accuracy: 0.8066\n",
            "Epoch 116/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1770 - accuracy: 0.9432 - val_loss: 1.1401 - val_accuracy: 0.8229\n",
            "Epoch 117/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1705 - accuracy: 0.9443 - val_loss: 1.0695 - val_accuracy: 0.8261\n",
            "Epoch 118/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1688 - accuracy: 0.9452 - val_loss: 1.1634 - val_accuracy: 0.8237\n",
            "Epoch 119/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1690 - accuracy: 0.9456 - val_loss: 1.0209 - val_accuracy: 0.8218\n",
            "Epoch 120/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1730 - accuracy: 0.9433 - val_loss: 1.1814 - val_accuracy: 0.8183\n",
            "Epoch 121/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1696 - accuracy: 0.9444 - val_loss: 1.0821 - val_accuracy: 0.8297\n",
            "Epoch 122/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1853 - accuracy: 0.9404 - val_loss: 1.0526 - val_accuracy: 0.8193\n",
            "Epoch 123/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1737 - accuracy: 0.9449 - val_loss: 1.1173 - val_accuracy: 0.8244\n",
            "Epoch 124/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1804 - accuracy: 0.9412 - val_loss: 1.1255 - val_accuracy: 0.8160\n",
            "Epoch 125/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1761 - accuracy: 0.9433 - val_loss: 1.0402 - val_accuracy: 0.8281\n",
            "Epoch 126/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1705 - accuracy: 0.9447 - val_loss: 1.2013 - val_accuracy: 0.8245\n",
            "Epoch 127/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1773 - accuracy: 0.9430 - val_loss: 0.9888 - val_accuracy: 0.8293\n",
            "Epoch 128/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1726 - accuracy: 0.9444 - val_loss: 1.0552 - val_accuracy: 0.8308\n",
            "Epoch 129/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1693 - accuracy: 0.9450 - val_loss: 1.1900 - val_accuracy: 0.8299\n",
            "Epoch 130/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1631 - accuracy: 0.9476 - val_loss: 1.1433 - val_accuracy: 0.8323\n",
            "Epoch 131/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1701 - accuracy: 0.9457 - val_loss: 1.1363 - val_accuracy: 0.8295\n",
            "Epoch 132/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1732 - accuracy: 0.9453 - val_loss: 1.0565 - val_accuracy: 0.8219\n",
            "Epoch 133/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1698 - accuracy: 0.9455 - val_loss: 1.0626 - val_accuracy: 0.8277\n",
            "Epoch 134/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1691 - accuracy: 0.9459 - val_loss: 1.0392 - val_accuracy: 0.8346\n",
            "Epoch 135/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1676 - accuracy: 0.9465 - val_loss: 1.1283 - val_accuracy: 0.8346\n",
            "Epoch 136/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1710 - accuracy: 0.9451 - val_loss: 1.2221 - val_accuracy: 0.8163\n",
            "Epoch 137/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1809 - accuracy: 0.9425 - val_loss: 1.0933 - val_accuracy: 0.8294\n",
            "Epoch 138/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1821 - accuracy: 0.9420 - val_loss: 1.0601 - val_accuracy: 0.8288\n",
            "Epoch 139/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1814 - accuracy: 0.9419 - val_loss: 1.0864 - val_accuracy: 0.8322\n",
            "Epoch 140/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1649 - accuracy: 0.9473 - val_loss: 1.1910 - val_accuracy: 0.8351\n",
            "Epoch 141/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1678 - accuracy: 0.9461 - val_loss: 1.0916 - val_accuracy: 0.8326\n",
            "Epoch 142/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1722 - accuracy: 0.9448 - val_loss: 1.2038 - val_accuracy: 0.8204\n",
            "Epoch 143/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1720 - accuracy: 0.9461 - val_loss: 1.0790 - val_accuracy: 0.8378\n",
            "Epoch 144/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1740 - accuracy: 0.9457 - val_loss: 1.1270 - val_accuracy: 0.8281\n",
            "Epoch 145/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1791 - accuracy: 0.9424 - val_loss: 1.0474 - val_accuracy: 0.8374\n",
            "Epoch 146/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1675 - accuracy: 0.9460 - val_loss: 1.1707 - val_accuracy: 0.8191\n",
            "Epoch 147/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1665 - accuracy: 0.9468 - val_loss: 1.3867 - val_accuracy: 0.8216\n",
            "Epoch 148/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1729 - accuracy: 0.9453 - val_loss: 0.9160 - val_accuracy: 0.8235\n",
            "Epoch 149/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1679 - accuracy: 0.9459 - val_loss: 1.0920 - val_accuracy: 0.8326\n",
            "Epoch 150/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1678 - accuracy: 0.9463 - val_loss: 1.1450 - val_accuracy: 0.8169\n",
            "Epoch 151/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1689 - accuracy: 0.9467 - val_loss: 1.3042 - val_accuracy: 0.8158\n",
            "Epoch 152/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1692 - accuracy: 0.9466 - val_loss: 1.1514 - val_accuracy: 0.8297\n",
            "Epoch 153/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1705 - accuracy: 0.9465 - val_loss: 1.0657 - val_accuracy: 0.8327\n",
            "Epoch 154/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1634 - accuracy: 0.9488 - val_loss: 1.2836 - val_accuracy: 0.8212\n",
            "Epoch 155/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1620 - accuracy: 0.9478 - val_loss: 1.2935 - val_accuracy: 0.8196\n",
            "Epoch 156/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1738 - accuracy: 0.9453 - val_loss: 1.1845 - val_accuracy: 0.8274\n",
            "Epoch 157/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1726 - accuracy: 0.9454 - val_loss: 1.2347 - val_accuracy: 0.8228\n",
            "Epoch 158/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1865 - accuracy: 0.9421 - val_loss: 0.9704 - val_accuracy: 0.8257\n",
            "Epoch 159/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1615 - accuracy: 0.9478 - val_loss: 1.1440 - val_accuracy: 0.8303\n",
            "Epoch 160/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1724 - accuracy: 0.9451 - val_loss: 1.2064 - val_accuracy: 0.8283\n",
            "Epoch 161/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1713 - accuracy: 0.9460 - val_loss: 1.1007 - val_accuracy: 0.8266\n",
            "Epoch 162/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1617 - accuracy: 0.9480 - val_loss: 1.2751 - val_accuracy: 0.8218\n",
            "Epoch 163/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1636 - accuracy: 0.9487 - val_loss: 1.2466 - val_accuracy: 0.8258\n",
            "Epoch 164/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1669 - accuracy: 0.9464 - val_loss: 1.0631 - val_accuracy: 0.8266\n",
            "Epoch 165/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1696 - accuracy: 0.9470 - val_loss: 1.1515 - val_accuracy: 0.8126\n",
            "Epoch 166/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1593 - accuracy: 0.9498 - val_loss: 1.2481 - val_accuracy: 0.8299\n",
            "Epoch 167/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1672 - accuracy: 0.9469 - val_loss: 1.1807 - val_accuracy: 0.8167\n",
            "Epoch 168/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1679 - accuracy: 0.9479 - val_loss: 1.2874 - val_accuracy: 0.8087\n",
            "Epoch 169/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1636 - accuracy: 0.9499 - val_loss: 1.1636 - val_accuracy: 0.8284\n",
            "Epoch 170/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1536 - accuracy: 0.9522 - val_loss: 1.1685 - val_accuracy: 0.8219\n",
            "Epoch 171/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1658 - accuracy: 0.9474 - val_loss: 1.0396 - val_accuracy: 0.8189\n",
            "Epoch 172/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1626 - accuracy: 0.9483 - val_loss: 1.1490 - val_accuracy: 0.8221\n",
            "Epoch 173/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1655 - accuracy: 0.9476 - val_loss: 1.3671 - val_accuracy: 0.8275\n",
            "Epoch 174/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1763 - accuracy: 0.9454 - val_loss: 1.1691 - val_accuracy: 0.8204\n",
            "Epoch 175/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1598 - accuracy: 0.9496 - val_loss: 1.1795 - val_accuracy: 0.8269\n",
            "Epoch 176/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1651 - accuracy: 0.9483 - val_loss: 1.1681 - val_accuracy: 0.8251\n",
            "Epoch 177/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1725 - accuracy: 0.9456 - val_loss: 1.1904 - val_accuracy: 0.8283\n",
            "Epoch 178/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1614 - accuracy: 0.9501 - val_loss: 1.1933 - val_accuracy: 0.8276\n",
            "Epoch 179/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1600 - accuracy: 0.9507 - val_loss: 1.1259 - val_accuracy: 0.8199\n",
            "Epoch 180/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1641 - accuracy: 0.9482 - val_loss: 1.1761 - val_accuracy: 0.8242\n",
            "Epoch 181/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1710 - accuracy: 0.9469 - val_loss: 1.1122 - val_accuracy: 0.8221\n",
            "Epoch 182/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1613 - accuracy: 0.9486 - val_loss: 1.2551 - val_accuracy: 0.8274\n",
            "Epoch 183/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1701 - accuracy: 0.9478 - val_loss: 1.2695 - val_accuracy: 0.8311\n",
            "Epoch 184/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1697 - accuracy: 0.9476 - val_loss: 1.2558 - val_accuracy: 0.8291\n",
            "Epoch 185/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1555 - accuracy: 0.9518 - val_loss: 1.1283 - val_accuracy: 0.8221\n",
            "Epoch 186/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1688 - accuracy: 0.9481 - val_loss: 1.2882 - val_accuracy: 0.8201\n",
            "Epoch 187/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1669 - accuracy: 0.9488 - val_loss: 1.1081 - val_accuracy: 0.8232\n",
            "Epoch 188/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1614 - accuracy: 0.9499 - val_loss: 1.2508 - val_accuracy: 0.8131\n",
            "Epoch 189/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1763 - accuracy: 0.9462 - val_loss: 1.2449 - val_accuracy: 0.8201\n",
            "Epoch 190/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1701 - accuracy: 0.9468 - val_loss: 1.2056 - val_accuracy: 0.8191\n",
            "Epoch 191/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1633 - accuracy: 0.9502 - val_loss: 1.1765 - val_accuracy: 0.8276\n",
            "Epoch 192/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1629 - accuracy: 0.9505 - val_loss: 1.1546 - val_accuracy: 0.8312\n",
            "Epoch 193/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1639 - accuracy: 0.9492 - val_loss: 1.1775 - val_accuracy: 0.8252\n",
            "Epoch 194/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1646 - accuracy: 0.9481 - val_loss: 1.2465 - val_accuracy: 0.8164\n",
            "Epoch 195/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1657 - accuracy: 0.9478 - val_loss: 1.1555 - val_accuracy: 0.8115\n",
            "Epoch 196/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1592 - accuracy: 0.9507 - val_loss: 1.1627 - val_accuracy: 0.8231\n",
            "Epoch 197/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1553 - accuracy: 0.9516 - val_loss: 1.1109 - val_accuracy: 0.8291\n",
            "Epoch 198/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1634 - accuracy: 0.9498 - val_loss: 1.2879 - val_accuracy: 0.8226\n",
            "Epoch 199/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1654 - accuracy: 0.9490 - val_loss: 1.2660 - val_accuracy: 0.8233\n",
            "Epoch 200/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1614 - accuracy: 0.9515 - val_loss: 1.0577 - val_accuracy: 0.8296\n",
            "Epoch 201/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1521 - accuracy: 0.9520 - val_loss: 1.0937 - val_accuracy: 0.8354\n",
            "Epoch 202/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1508 - accuracy: 0.9527 - val_loss: 1.2732 - val_accuracy: 0.8317\n",
            "Epoch 203/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1581 - accuracy: 0.9514 - val_loss: 1.1917 - val_accuracy: 0.8318\n",
            "Epoch 204/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1518 - accuracy: 0.9530 - val_loss: 1.2763 - val_accuracy: 0.8303\n",
            "Epoch 205/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1714 - accuracy: 0.9480 - val_loss: 1.3824 - val_accuracy: 0.8195\n",
            "Epoch 206/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1625 - accuracy: 0.9504 - val_loss: 1.1342 - val_accuracy: 0.8307\n",
            "Epoch 207/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1551 - accuracy: 0.9514 - val_loss: 1.2439 - val_accuracy: 0.8327\n",
            "Epoch 208/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1621 - accuracy: 0.9489 - val_loss: 1.3172 - val_accuracy: 0.8288\n",
            "Epoch 209/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1534 - accuracy: 0.9537 - val_loss: 1.2109 - val_accuracy: 0.8263\n",
            "Epoch 210/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1500 - accuracy: 0.9544 - val_loss: 1.2865 - val_accuracy: 0.8298\n",
            "Epoch 211/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1588 - accuracy: 0.9517 - val_loss: 1.4079 - val_accuracy: 0.8243\n",
            "Epoch 212/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1641 - accuracy: 0.9495 - val_loss: 1.1545 - val_accuracy: 0.8193\n",
            "Epoch 213/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1443 - accuracy: 0.9558 - val_loss: 1.2804 - val_accuracy: 0.8318\n",
            "Epoch 214/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1492 - accuracy: 0.9545 - val_loss: 1.3407 - val_accuracy: 0.8222\n",
            "Epoch 215/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1553 - accuracy: 0.9527 - val_loss: 1.1362 - val_accuracy: 0.8267\n",
            "Epoch 216/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1600 - accuracy: 0.9519 - val_loss: 1.1608 - val_accuracy: 0.8256\n",
            "Epoch 217/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1525 - accuracy: 0.9528 - val_loss: 1.2840 - val_accuracy: 0.8251\n",
            "Epoch 218/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1524 - accuracy: 0.9534 - val_loss: 1.4246 - val_accuracy: 0.8183\n",
            "Epoch 219/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1482 - accuracy: 0.9546 - val_loss: 1.2738 - val_accuracy: 0.8236\n",
            "Epoch 220/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1597 - accuracy: 0.9518 - val_loss: 1.2224 - val_accuracy: 0.8177\n",
            "Epoch 221/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1539 - accuracy: 0.9532 - val_loss: 1.2511 - val_accuracy: 0.8280\n",
            "Epoch 222/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1485 - accuracy: 0.9541 - val_loss: 1.2653 - val_accuracy: 0.8287\n",
            "Epoch 223/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1569 - accuracy: 0.9528 - val_loss: 1.2224 - val_accuracy: 0.8214\n",
            "Epoch 224/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1525 - accuracy: 0.9534 - val_loss: 1.3960 - val_accuracy: 0.8238\n",
            "Epoch 225/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1515 - accuracy: 0.9542 - val_loss: 1.1075 - val_accuracy: 0.8310\n",
            "Epoch 226/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1453 - accuracy: 0.9559 - val_loss: 1.4399 - val_accuracy: 0.8199\n",
            "Epoch 227/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1431 - accuracy: 0.9566 - val_loss: 1.2828 - val_accuracy: 0.8271\n",
            "Epoch 228/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1612 - accuracy: 0.9511 - val_loss: 1.1391 - val_accuracy: 0.8319\n",
            "Epoch 229/350\n",
            "1563/1563 [==============================] - 26s 16ms/step - loss: 0.1459 - accuracy: 0.9553 - val_loss: 1.2205 - val_accuracy: 0.8317\n",
            "Epoch 230/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1599 - accuracy: 0.9520 - val_loss: 1.2999 - val_accuracy: 0.8225\n",
            "Epoch 231/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1548 - accuracy: 0.9523 - val_loss: 1.3489 - val_accuracy: 0.8188\n",
            "Epoch 232/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1506 - accuracy: 0.9529 - val_loss: 1.2650 - val_accuracy: 0.8285\n",
            "Epoch 233/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1569 - accuracy: 0.9533 - val_loss: 1.2579 - val_accuracy: 0.8279\n",
            "Epoch 234/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1515 - accuracy: 0.9540 - val_loss: 1.2913 - val_accuracy: 0.8257\n",
            "Epoch 235/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1546 - accuracy: 0.9532 - val_loss: 1.2276 - val_accuracy: 0.8292\n",
            "Epoch 236/350\n",
            "1563/1563 [==============================] - 27s 18ms/step - loss: 0.1527 - accuracy: 0.9535 - val_loss: 1.2816 - val_accuracy: 0.8243\n",
            "Epoch 237/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1521 - accuracy: 0.9530 - val_loss: 1.3047 - val_accuracy: 0.8327\n",
            "Epoch 238/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1414 - accuracy: 0.9562 - val_loss: 1.3107 - val_accuracy: 0.8256\n",
            "Epoch 239/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1494 - accuracy: 0.9549 - val_loss: 1.3186 - val_accuracy: 0.8278\n",
            "Epoch 240/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1479 - accuracy: 0.9543 - val_loss: 1.3010 - val_accuracy: 0.8271\n",
            "Epoch 241/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1575 - accuracy: 0.9517 - val_loss: 1.2206 - val_accuracy: 0.8246\n",
            "Epoch 242/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1520 - accuracy: 0.9549 - val_loss: 1.2738 - val_accuracy: 0.8293\n",
            "Epoch 243/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1484 - accuracy: 0.9551 - val_loss: 1.2214 - val_accuracy: 0.8191\n",
            "Epoch 244/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1521 - accuracy: 0.9543 - val_loss: 1.1692 - val_accuracy: 0.8311\n",
            "Epoch 245/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1482 - accuracy: 0.9551 - val_loss: 1.1415 - val_accuracy: 0.8250\n",
            "Epoch 246/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1501 - accuracy: 0.9543 - val_loss: 1.3030 - val_accuracy: 0.8259\n",
            "Epoch 247/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1526 - accuracy: 0.9547 - val_loss: 1.2857 - val_accuracy: 0.8358\n",
            "Epoch 248/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1474 - accuracy: 0.9547 - val_loss: 1.2635 - val_accuracy: 0.8249\n",
            "Epoch 249/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1411 - accuracy: 0.9570 - val_loss: 1.2348 - val_accuracy: 0.8338\n",
            "Epoch 250/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1440 - accuracy: 0.9569 - val_loss: 1.2078 - val_accuracy: 0.8280\n",
            "Epoch 251/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1476 - accuracy: 0.9550 - val_loss: 1.2607 - val_accuracy: 0.8341\n",
            "Epoch 252/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1483 - accuracy: 0.9555 - val_loss: 1.3573 - val_accuracy: 0.8305\n",
            "Epoch 253/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1539 - accuracy: 0.9536 - val_loss: 1.2535 - val_accuracy: 0.8270\n",
            "Epoch 254/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1473 - accuracy: 0.9558 - val_loss: 1.2200 - val_accuracy: 0.8362\n",
            "Epoch 255/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1447 - accuracy: 0.9568 - val_loss: 1.2821 - val_accuracy: 0.8299\n",
            "Epoch 256/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1478 - accuracy: 0.9554 - val_loss: 1.2574 - val_accuracy: 0.8351\n",
            "Epoch 257/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1423 - accuracy: 0.9586 - val_loss: 1.1756 - val_accuracy: 0.8352\n",
            "Epoch 258/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1298 - accuracy: 0.9604 - val_loss: 1.3474 - val_accuracy: 0.8299\n",
            "Epoch 259/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1428 - accuracy: 0.9564 - val_loss: 1.2770 - val_accuracy: 0.8288\n",
            "Epoch 260/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1451 - accuracy: 0.9562 - val_loss: 1.2517 - val_accuracy: 0.8268\n",
            "Epoch 261/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1461 - accuracy: 0.9568 - val_loss: 1.3682 - val_accuracy: 0.8238\n",
            "Epoch 262/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1433 - accuracy: 0.9560 - val_loss: 1.3121 - val_accuracy: 0.8266\n",
            "Epoch 263/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1378 - accuracy: 0.9579 - val_loss: 1.4308 - val_accuracy: 0.8312\n",
            "Epoch 264/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1529 - accuracy: 0.9549 - val_loss: 1.3119 - val_accuracy: 0.8143\n",
            "Epoch 265/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1388 - accuracy: 0.9579 - val_loss: 1.2246 - val_accuracy: 0.8274\n",
            "Epoch 266/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1352 - accuracy: 0.9596 - val_loss: 1.4465 - val_accuracy: 0.8189\n",
            "Epoch 267/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1483 - accuracy: 0.9562 - val_loss: 1.3932 - val_accuracy: 0.8321\n",
            "Epoch 268/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1597 - accuracy: 0.9526 - val_loss: 1.2652 - val_accuracy: 0.8244\n",
            "Epoch 269/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1483 - accuracy: 0.9548 - val_loss: 1.4305 - val_accuracy: 0.8241\n",
            "Epoch 270/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1446 - accuracy: 0.9566 - val_loss: 1.3691 - val_accuracy: 0.8193\n",
            "Epoch 271/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1463 - accuracy: 0.9561 - val_loss: 1.4916 - val_accuracy: 0.8242\n",
            "Epoch 272/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1474 - accuracy: 0.9561 - val_loss: 1.3368 - val_accuracy: 0.8318\n",
            "Epoch 273/350\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 0.1328 - accuracy: 0.9600 - val_loss: 1.5570 - val_accuracy: 0.8248\n",
            "Epoch 274/350\n",
            "1563/1563 [==============================] - 27s 18ms/step - loss: 0.1507 - accuracy: 0.9554 - val_loss: 1.3219 - val_accuracy: 0.8313\n",
            "Epoch 275/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1475 - accuracy: 0.9570 - val_loss: 1.2745 - val_accuracy: 0.8255\n",
            "Epoch 276/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1416 - accuracy: 0.9584 - val_loss: 1.3096 - val_accuracy: 0.8296\n",
            "Epoch 277/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1377 - accuracy: 0.9595 - val_loss: 1.2871 - val_accuracy: 0.8267\n",
            "Epoch 278/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1445 - accuracy: 0.9571 - val_loss: 1.3820 - val_accuracy: 0.8188\n",
            "Epoch 279/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1444 - accuracy: 0.9570 - val_loss: 1.3774 - val_accuracy: 0.8260\n",
            "Epoch 280/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1274 - accuracy: 0.9614 - val_loss: 1.4201 - val_accuracy: 0.8253\n",
            "Epoch 281/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1339 - accuracy: 0.9609 - val_loss: 1.3937 - val_accuracy: 0.8256\n",
            "Epoch 282/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1408 - accuracy: 0.9587 - val_loss: 1.3595 - val_accuracy: 0.8204\n",
            "Epoch 283/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1372 - accuracy: 0.9588 - val_loss: 1.3394 - val_accuracy: 0.8353\n",
            "Epoch 284/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1315 - accuracy: 0.9608 - val_loss: 1.4928 - val_accuracy: 0.8296\n",
            "Epoch 285/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1345 - accuracy: 0.9603 - val_loss: 1.3868 - val_accuracy: 0.8263\n",
            "Epoch 286/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1422 - accuracy: 0.9584 - val_loss: 1.4179 - val_accuracy: 0.8248\n",
            "Epoch 287/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1268 - accuracy: 0.9617 - val_loss: 1.5711 - val_accuracy: 0.8194\n",
            "Epoch 288/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1444 - accuracy: 0.9569 - val_loss: 1.3004 - val_accuracy: 0.8309\n",
            "Epoch 289/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1423 - accuracy: 0.9585 - val_loss: 1.3315 - val_accuracy: 0.8348\n",
            "Epoch 290/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1410 - accuracy: 0.9583 - val_loss: 1.3670 - val_accuracy: 0.8328\n",
            "Epoch 291/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1313 - accuracy: 0.9610 - val_loss: 1.3632 - val_accuracy: 0.8341\n",
            "Epoch 292/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1325 - accuracy: 0.9610 - val_loss: 1.3016 - val_accuracy: 0.8276\n",
            "Epoch 293/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1337 - accuracy: 0.9608 - val_loss: 1.4969 - val_accuracy: 0.8203\n",
            "Epoch 294/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1347 - accuracy: 0.9596 - val_loss: 1.6105 - val_accuracy: 0.8245\n",
            "Epoch 295/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1377 - accuracy: 0.9587 - val_loss: 1.3524 - val_accuracy: 0.8356\n",
            "Epoch 296/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1320 - accuracy: 0.9608 - val_loss: 1.4426 - val_accuracy: 0.8322\n",
            "Epoch 297/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1390 - accuracy: 0.9593 - val_loss: 1.3217 - val_accuracy: 0.8277\n",
            "Epoch 298/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1333 - accuracy: 0.9607 - val_loss: 1.4429 - val_accuracy: 0.8171\n",
            "Epoch 299/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1357 - accuracy: 0.9606 - val_loss: 1.3481 - val_accuracy: 0.8273\n",
            "Epoch 300/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1351 - accuracy: 0.9605 - val_loss: 1.4205 - val_accuracy: 0.8312\n",
            "Epoch 301/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1420 - accuracy: 0.9580 - val_loss: 1.3713 - val_accuracy: 0.8257\n",
            "Epoch 302/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1440 - accuracy: 0.9575 - val_loss: 1.2467 - val_accuracy: 0.8330\n",
            "Epoch 303/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1318 - accuracy: 0.9623 - val_loss: 1.3623 - val_accuracy: 0.8359\n",
            "Epoch 304/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1272 - accuracy: 0.9633 - val_loss: 1.5768 - val_accuracy: 0.8232\n",
            "Epoch 305/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1417 - accuracy: 0.9587 - val_loss: 1.3229 - val_accuracy: 0.8278\n",
            "Epoch 306/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1349 - accuracy: 0.9605 - val_loss: 1.3634 - val_accuracy: 0.8308\n",
            "Epoch 307/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1318 - accuracy: 0.9622 - val_loss: 1.3325 - val_accuracy: 0.8307\n",
            "Epoch 308/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1296 - accuracy: 0.9615 - val_loss: 1.4522 - val_accuracy: 0.8294\n",
            "Epoch 309/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1436 - accuracy: 0.9600 - val_loss: 1.3190 - val_accuracy: 0.8267\n",
            "Epoch 310/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1334 - accuracy: 0.9609 - val_loss: 1.4027 - val_accuracy: 0.8278\n",
            "Epoch 311/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1419 - accuracy: 0.9582 - val_loss: 1.2506 - val_accuracy: 0.8278\n",
            "Epoch 312/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1318 - accuracy: 0.9605 - val_loss: 1.4671 - val_accuracy: 0.8352\n",
            "Epoch 313/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1298 - accuracy: 0.9614 - val_loss: 1.4344 - val_accuracy: 0.8286\n",
            "Epoch 314/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1375 - accuracy: 0.9594 - val_loss: 1.5250 - val_accuracy: 0.8282\n",
            "Epoch 315/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1407 - accuracy: 0.9596 - val_loss: 1.2144 - val_accuracy: 0.8216\n",
            "Epoch 316/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1334 - accuracy: 0.9615 - val_loss: 1.2572 - val_accuracy: 0.8305\n",
            "Epoch 317/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1234 - accuracy: 0.9640 - val_loss: 1.4154 - val_accuracy: 0.8284\n",
            "Epoch 318/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1459 - accuracy: 0.9571 - val_loss: 1.4722 - val_accuracy: 0.8330\n",
            "Epoch 319/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1363 - accuracy: 0.9610 - val_loss: 1.4199 - val_accuracy: 0.8268\n",
            "Epoch 320/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1268 - accuracy: 0.9626 - val_loss: 1.4002 - val_accuracy: 0.8293\n",
            "Epoch 321/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1292 - accuracy: 0.9627 - val_loss: 1.4466 - val_accuracy: 0.8236\n",
            "Epoch 322/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1371 - accuracy: 0.9605 - val_loss: 1.3423 - val_accuracy: 0.8259\n",
            "Epoch 323/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1121 - accuracy: 0.9676 - val_loss: 1.6414 - val_accuracy: 0.8271\n",
            "Epoch 324/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1286 - accuracy: 0.9640 - val_loss: 1.3381 - val_accuracy: 0.8342\n",
            "Epoch 325/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1215 - accuracy: 0.9640 - val_loss: 1.4139 - val_accuracy: 0.8241\n",
            "Epoch 326/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1310 - accuracy: 0.9616 - val_loss: 1.4615 - val_accuracy: 0.8238\n",
            "Epoch 327/350\n",
            "1563/1563 [==============================] - 27s 18ms/step - loss: 0.1301 - accuracy: 0.9621 - val_loss: 1.3341 - val_accuracy: 0.8239\n",
            "Epoch 328/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1163 - accuracy: 0.9656 - val_loss: 1.3552 - val_accuracy: 0.8277\n",
            "Epoch 329/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1206 - accuracy: 0.9637 - val_loss: 1.5964 - val_accuracy: 0.8262\n",
            "Epoch 330/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1360 - accuracy: 0.9616 - val_loss: 1.3809 - val_accuracy: 0.8288\n",
            "Epoch 331/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1329 - accuracy: 0.9619 - val_loss: 1.5441 - val_accuracy: 0.8248\n",
            "Epoch 332/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1333 - accuracy: 0.9615 - val_loss: 1.3504 - val_accuracy: 0.8304\n",
            "Epoch 333/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1227 - accuracy: 0.9642 - val_loss: 1.3576 - val_accuracy: 0.8223\n",
            "Epoch 334/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1247 - accuracy: 0.9641 - val_loss: 1.4014 - val_accuracy: 0.8279\n",
            "Epoch 335/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1222 - accuracy: 0.9643 - val_loss: 1.4701 - val_accuracy: 0.8331\n",
            "Epoch 336/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1197 - accuracy: 0.9653 - val_loss: 1.4029 - val_accuracy: 0.8319\n",
            "Epoch 337/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1259 - accuracy: 0.9634 - val_loss: 1.6004 - val_accuracy: 0.8258\n",
            "Epoch 338/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1300 - accuracy: 0.9627 - val_loss: 1.4255 - val_accuracy: 0.8296\n",
            "Epoch 339/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1234 - accuracy: 0.9637 - val_loss: 1.3461 - val_accuracy: 0.8343\n",
            "Epoch 340/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1106 - accuracy: 0.9669 - val_loss: 1.3963 - val_accuracy: 0.8349\n",
            "Epoch 341/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1271 - accuracy: 0.9637 - val_loss: 1.6853 - val_accuracy: 0.8310\n",
            "Epoch 342/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1256 - accuracy: 0.9649 - val_loss: 1.4085 - val_accuracy: 0.8298\n",
            "Epoch 343/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1205 - accuracy: 0.9646 - val_loss: 1.4361 - val_accuracy: 0.8286\n",
            "Epoch 344/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1097 - accuracy: 0.9682 - val_loss: 1.3853 - val_accuracy: 0.8336\n",
            "Epoch 345/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1186 - accuracy: 0.9653 - val_loss: 1.5264 - val_accuracy: 0.8343\n",
            "Epoch 346/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1276 - accuracy: 0.9640 - val_loss: 1.5151 - val_accuracy: 0.8272\n",
            "Epoch 347/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1189 - accuracy: 0.9652 - val_loss: 1.2866 - val_accuracy: 0.8324\n",
            "Epoch 348/350\n",
            "1563/1563 [==============================] - 27s 17ms/step - loss: 0.1215 - accuracy: 0.9652 - val_loss: 1.4768 - val_accuracy: 0.8244\n",
            "Epoch 349/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1165 - accuracy: 0.9656 - val_loss: 1.3866 - val_accuracy: 0.8315\n",
            "Epoch 350/350\n",
            "1563/1563 [==============================] - 28s 18ms/step - loss: 0.1144 - accuracy: 0.9675 - val_loss: 1.4144 - val_accuracy: 0.8289\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2d600a9590>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#making predictions\n"
      ],
      "metadata": {
        "id": "3nnJc1s8TTPf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make dictionary of class labels and names\n",
        "classes = range(0,10)\n",
        "\n",
        "names = ['airplane',\n",
        "        'automobile',\n",
        "        'bird',\n",
        "        'cat',\n",
        "        'deer',\n",
        "        'dog',\n",
        "        'frog',\n",
        "        'horse',\n",
        "        'ship',\n",
        "        'truck']\n",
        "\n",
        "# zip the names and classes to make a dictionary of class_labels\n",
        "class_labels = dict(zip(classes, names))\n",
        "\n",
        "# generate batch of 9 images to predict\n",
        "batch = X_test[100:109]\n",
        "labels = np.argmax(Y_test[100:109],axis=-1)\n",
        "\n",
        "# make predictions\n",
        "predictions = model.predict(batch, verbose = 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR7cbImQvHLi",
        "outputId": "e22142ff-fa87-4837-cb79-02c4e1f2198c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print our predictions\n",
        "print (predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7r25SSHvMKB",
        "outputId": "9dbd8c51-ce5f-4b9c-b4ef-665f56378c3f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.85374380e-22 5.85538748e-27 2.04229967e-16 4.36437882e-16\n",
            "  1.00000000e+00 3.79707116e-12 1.22906418e-15 6.00052186e-09\n",
            "  1.25689170e-22 1.20390572e-20]\n",
            " [1.80133240e-18 2.62557492e-19 4.55800560e-04 1.63415517e-03\n",
            "  2.80371182e-06 9.97889340e-01 7.97645043e-06 9.91463821e-06\n",
            "  2.44860013e-14 3.44172022e-12]\n",
            " [2.79520453e-36 1.38494579e-22 4.22009509e-22 1.11884862e-24\n",
            "  1.21196645e-25 5.41917978e-24 1.00000000e+00 2.72636677e-26\n",
            "  4.57538988e-28 9.09623862e-26]\n",
            " [1.62990873e-32 3.00930558e-33 7.56990889e-15 9.99999642e-01\n",
            "  1.54090650e-26 1.24649013e-17 4.10378192e-07 4.67267479e-24\n",
            "  4.97115209e-32 1.87957820e-27]\n",
            " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.07959709e-17 6.25015551e-19 2.98194750e-03 9.88252819e-01\n",
            "  4.32232592e-12 6.44575607e-07 8.76463298e-03 1.40542682e-11\n",
            "  1.40820035e-16 8.49810487e-17]\n",
            " [0.00000000e+00 1.82907505e-28 1.62287704e-28 1.06100686e-32\n",
            "  1.32727648e-33 2.12647020e-37 1.00000000e+00 0.00000000e+00\n",
            "  1.80580597e-38 2.22606212e-33]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for image in predictions:\n",
        "    print(np.sum(image))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3pGQH7QvWjG",
        "outputId": "e35cdbb7-3a61-4630-81ad-cea77566a220"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_result = np.argmax(predictions,axis=-1)\n",
        "print(class_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INxBdCyHvkH9",
        "outputId": "7f9220c0-c45e-4a6c-d85b-1830521d7fdd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4 5 6 3 1 1 3 6 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a grid of 3x3 images\n",
        "fig, axs = plt.subplots(3, 3, figsize = (15, 6))\n",
        "fig.subplots_adjust(hspace = 1)\n",
        "axs = axs.flatten()\n",
        "\n",
        "for i, img in enumerate(batch):\n",
        "\n",
        "    # determine label for each prediction, set title\n",
        "    for key, value in class_labels.items():\n",
        "        if class_result[i] == key:\n",
        "            title = 'Prediction: {}\\nActual: {}'.format(class_labels[key], class_labels[labels[i]])\n",
        "            axs[i].set_title(title)\n",
        "            axs[i].axes.get_xaxis().set_visible(False)\n",
        "            axs[i].axes.get_yaxis().set_visible(False)\n",
        "            \n",
        "    # plot the image\n",
        "    axs[i].imshow(img.transpose([1,2,0]))\n",
        "    \n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "id": "xImBz_H9vnGO",
        "outputId": "95a516a6-49b8-47d9-c77e-e8958af44a89"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-fc13be9b8f6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# plot the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# show the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    698\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 699\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (32, 3, 32) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x432 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fk1gP3B5vvIl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}